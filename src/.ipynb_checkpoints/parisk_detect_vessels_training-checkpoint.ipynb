{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plaq-u-net: multi-patch consensus U-Net for automated detection and segmentation of the carotid arteries on black blood MRI sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. Lavrova, 2022  \n",
    "  \n",
    "This is a code supporting the corresponding paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from xml.etree import ElementTree\n",
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "import pydicom\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "from skimage import morphology\n",
    "from skimage.filters import threshold_otsu, threshold_local\n",
    "\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2,3'                        \n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting image/contour names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob('Z:/Lisa/PARISK/PARISK-XNAT/plaque_converted_unempty slices/*/T1W_*.dcm')\n",
    "contours = glob.glob('Z:/Lisa/PARISK/PARISK-XNAT/plaque_converted_unempty slices/*/MASSExport/*.dcm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of image/contour files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(images), len(contours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting patients with contours presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_names_list = []\n",
    "for image in images:\n",
    "    sub_name = image.split(os.sep)[1].split('_')[0]\n",
    "    con_names = glob.glob(os.path.join(os.path.split(image)[0],'MASSExport')+os.sep+'*'+ image.split(os.sep)[2][-10:-4]+'*.dcm')\n",
    "    if len(con_names)>0:\n",
    "        sub_names_list.append(sub_name)\n",
    "        \n",
    "sub_names = np.unique(sub_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting subs into centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_names_amc = []\n",
    "sub_names_mumc = []\n",
    "sub_names_umcu = []\n",
    "sub_names_emc = []\n",
    "\n",
    "for sub_name in sub_names:\n",
    "    if 'AMC' in sub_name:\n",
    "        sub_names_amc.append(sub_name)\n",
    "    if 'MUMC' in sub_name:\n",
    "        sub_names_mumc.append(sub_name)\n",
    "    if 'UMCU' in sub_name:\n",
    "        sub_names_umcu.append(sub_name)\n",
    "    if 'EMC' in sub_name:\n",
    "        sub_names_emc.append(sub_name)\n",
    "        \n",
    "print (len(sub_names_amc), len(sub_names_mumc), len(sub_names_umcu), len(sub_names_emc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data split for training, validation, and testing stratified per center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_names_amc_train, sub_names_amc_valid_test = train_test_split(sub_names_amc, test_size=0.3, \n",
    "                                                                 random_state=2022, shuffle=True)\n",
    "sub_names_amc_valid, sub_names_amc_test = train_test_split(sub_names_amc_valid_test, test_size=0.5, \n",
    "                                                           random_state=2022, shuffle=True)\n",
    "\n",
    "sub_names_mumc_train, sub_names_mumc_valid_test = train_test_split(sub_names_mumc, test_size=0.3, \n",
    "                                                                 random_state=2022, shuffle=True)\n",
    "sub_names_mumc_valid, sub_names_mumc_test = train_test_split(sub_names_mumc_valid_test, test_size=0.5, \n",
    "                                                           random_state=2022, shuffle=True)\n",
    "\n",
    "sub_names_umcu_train, sub_names_umcu_valid_test = train_test_split(sub_names_umcu, test_size=0.3, \n",
    "                                                                 random_state=2022, shuffle=True)\n",
    "sub_names_umcu_valid, sub_names_umcu_test = train_test_split(sub_names_umcu_valid_test, test_size=0.5, \n",
    "                                                           random_state=2022, shuffle=True)\n",
    "\n",
    "sub_names_train = sub_names_amc_train + sub_names_mumc_train + sub_names_umcu_train\n",
    "sub_names_valid = sub_names_amc_valid + sub_names_mumc_valid + sub_names_umcu_valid\n",
    "sub_names_test = sub_names_amc_test + sub_names_mumc_test + sub_names_umcu_test\n",
    "\n",
    "print (len(sub_names_amc_train), len(sub_names_mumc_train), len(sub_names_umcu_train))\n",
    "print (len(sub_names_amc_valid), len(sub_names_mumc_valid), len(sub_names_umcu_valid))\n",
    "print (len(sub_names_amc_test), len(sub_names_mumc_test), len(sub_names_umcu_test))\n",
    "print (len(sub_names_emc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of patients in training, validation, and testing subsets per center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|DS | Total  | Train    | Valid   | Test   | Ext test |\n",
    "|---:|:-------------|:-----------|:------|:------|:-----|\n",
    "| AMC | 13  | 9 | 2   | 2     |0|\n",
    "| MUMC | 115  | 80    | 17   | 18     |0|\n",
    "| UMCU | 25  | 17    | 4   | 4     |0|  \n",
    "| EMC | 34|0|0|0|34|\n",
    "| Total | 187  | 106    | 23   | 24     |34|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting .dcm files for every subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names_train = []\n",
    "img_names_valid = []\n",
    "img_names_test = []\n",
    "\n",
    "for sub_name_train in sub_names_train:\n",
    "    img_names_train.extend(glob.glob('Z:/Lisa/PARISK/PARISK-XNAT/plaque_converted_unempty slices/'+sub_name_train+'*/T1W_*.dcm'))\n",
    "\n",
    "for sub_name_valid in sub_names_valid:\n",
    "    img_names_valid.extend(glob.glob('Z:/Lisa/PARISK/PARISK-XNAT/plaque_converted_unempty slices/'+sub_name_valid+'*/T1W_*.dcm'))\n",
    "    \n",
    "for sub_name_test in sub_names_test:\n",
    "    img_names_test.extend(glob.glob('Z:/Lisa/PARISK/PARISK-XNAT/plaque_converted_unempty slices/'+sub_name_test+'*/T1W_*.dcm'))\n",
    "    \n",
    "print (len(img_names_train), len(img_names_valid), len(img_names_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_side = 64\n",
    "im_width = im_side\n",
    "im_height = im_side\n",
    "border = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(img_names_train), im_side, im_side, 1), dtype = np.uint8)\n",
    "y_train = np.zeros((len(img_names_train), im_side, im_side, 1), dtype = np.uint8)\n",
    "\n",
    "X_valid = np.zeros((len(img_names_valid), im_side, im_side, 1), dtype = np.uint8)\n",
    "y_valid = np.zeros((len(img_names_valid), im_side, im_side, 1), dtype = np.uint8)\n",
    "\n",
    "X_test = np.zeros((len(img_names_test), im_side, im_side, 1), dtype = np.uint8)\n",
    "y_test = np.zeros((len(img_names_test), im_side, im_side, 1), dtype = np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2array(dcm_path):\n",
    "    arr_dcm = pydicom.read_file(dcm_path, force = True)\n",
    "    arr_dcm.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian\n",
    "    arr = arr_dcm.pixel_array\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_crop(img_arr, con_arr, im_side):\n",
    "    \n",
    "    img_min = np.min(img_arr) \n",
    "    img_max = np.max(img_arr) \n",
    "    \n",
    "    img_norm = np.copy((img_arr - img_min)/(img_max - img_min)*255).astype(np.uint8)\n",
    "\n",
    "    con_xmin = np.min(np.where(con_arr > 0)[0])\n",
    "    con_xmax = np.max(np.where(con_arr > 0)[0])\n",
    "    con_ymin = np.min(np.where(con_arr > 0)[1])\n",
    "    con_ymax = np.max(np.where(con_arr > 0)[1])\n",
    "        \n",
    "    side_x = con_xmax - con_xmin\n",
    "    side_y = con_ymax - con_ymin\n",
    "\n",
    "    crop_xmin = max(0, con_xmin-random.randint(2, max(2, im_side-side_x)))\n",
    "    crop_ymin = max(0, con_ymin-random.randint(2, max(2, im_side-side_y)))\n",
    "        \n",
    "    crop_xmax = crop_xmin + im_side\n",
    "    crop_ymax = crop_ymin + im_side\n",
    "\n",
    "    crop = [crop_xmin, crop_xmax, crop_ymin, crop_ymax]\n",
    "\n",
    "    img_norm_crop = np.copy(img_norm[crop[0]:crop[1], crop[2]:crop[3]])\n",
    "    con_crop = np.copy(con_arr[crop[0]:crop[1], crop[2]:crop[3]])\n",
    "\n",
    "    return img_norm_crop, con_crop>0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def crop_norm(img_arr, con_arr, im_side):\n",
    "\n",
    "#    con_xmin = np.min(np.where(con_arr > 0)[0])\n",
    "#    con_xmax = np.max(np.where(con_arr > 0)[0])\n",
    "#    con_ymin = np.min(np.where(con_arr > 0)[1])\n",
    "#    con_ymax = np.max(np.where(con_arr > 0)[1])\n",
    "        \n",
    "#    side_x = con_xmax - con_xmin\n",
    "#    side_y = con_ymax - con_ymin\n",
    "\n",
    "#    crop_xmin = max(0, con_xmin-random.randint(2, max(2, im_side-side_x)))\n",
    "#    crop_ymin = max(0, con_ymin-random.randint(2, max(2, im_side-side_y)))\n",
    "        \n",
    "#    crop_xmax = crop_xmin + im_side\n",
    "#    crop_ymax = crop_ymin + im_side\n",
    "\n",
    "#    crop = [crop_xmin, crop_xmax, crop_ymin, crop_ymax]\n",
    "\n",
    "#    img_crop = np.copy(img_arr[crop[0]:crop[1], crop[2]:crop[3]])\n",
    "#    con_crop = np.copy(con_arr[crop[0]:crop[1], crop[2]:crop[3]])\n",
    "    \n",
    "#    img_min = np.min(img_crop) \n",
    "#    img_max = np.max(img_crop)\n",
    "    \n",
    "#    img_crop_norm = np.copy((img_crop - img_min)/(img_max - img_min)*255).astype(np.uint8)\n",
    "\n",
    "#    return img_crop_norm, con_crop>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths2dataarray(paths_list, X_array, y_array, im_side):\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for impth in paths_list:\n",
    "\n",
    "        img = path2array(impth)\n",
    "        con_name = glob.glob(os.path.join(os.path.split(impth)[0],'MASSExport')+os.sep+'*'+ impth.split(os.sep)[2][-10:-4]+'*.dcm')[0]\n",
    "        con = path2array(con_name)\n",
    "        \n",
    "        img_min = np.min(img)\n",
    "        img_max = np.max(img)\n",
    "\n",
    "        if (np.sum(con)>0)&((img_max - img_min)>0):\n",
    "\n",
    "            img_prep, con_prep = norm_crop(img, con, im_side)\n",
    "            #img_prep, con_prep = crop_norm(img, con, im_side)\n",
    "\n",
    "            X_array[i, ..., 0] = img_prep\n",
    "            y_array[i, ..., 0] = con_prep\n",
    "\n",
    "            i+=1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths2dataarray(img_names_train, X_train, y_train, 64)\n",
    "paths2dataarray(img_names_valid, X_valid, y_valid, 64)\n",
    "paths2dataarray(img_names_test, X_test, y_test, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, 10):\n",
    "    plt.imshow(X_train[i, ..., 0], cmap = 'bone')\n",
    "    plt.imshow(y_train[i, ..., 0], alpha = 0.3)\n",
    "    plt.axis('Off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomSizedCrop(min_max_height=(48, 64), height=64, width=64, p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Blur(p=0.5),\n",
    "    A.GaussNoise(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=0.5),\n",
    "    A.RandomGamma(p=0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(0)\n",
    "\n",
    "transformed = transform(image=X_train[0, ..., 0], mask=y_train[0, ..., 0])\n",
    "transformed_image = transformed['image']\n",
    "transformed_mask = transformed['mask']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[0, ..., 0])\n",
    "plt.contour(y_train[0, ..., 0], colors='r')\n",
    "plt.subplot(122)\n",
    "plt.imshow(transformed_image)\n",
    "plt.contour(transformed_mask, colors='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_ds(X, y, n_augms):\n",
    "    \n",
    "    counter = 0\n",
    "    dim = y[0, ..., 0].shape\n",
    "    n_samples = y.shape[0]\n",
    "    \n",
    "    X_aug = np.zeros((int(n_samples*n_augms), dim[0], dim[1], 1), dtype = np.uint8)\n",
    "    y_aug = np.zeros((int(n_samples*n_augms), dim[0], dim[1], 1), dtype = np.uint8)\n",
    "    \n",
    "    for smpl in range (0, n_samples):\n",
    "        for augm in range (0, n_augms):\n",
    "            \n",
    "            random.seed(counter)\n",
    "            augmented = transform(image=X[smpl, ..., 0], mask=y[smpl, ..., 0])\n",
    "            X_aug[counter, ..., 0] = augmented['image']\n",
    "            y_aug[counter, ..., 0] = augmented['mask']\n",
    "            counter += 1\n",
    "            \n",
    "    return X_aug, y_aug    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug, y_train_aug = augment_ds(X_train, y_train, 10)\n",
    "X_valid_aug, y_valid_aug = augment_ds(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 5\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train[n_sample//10, ..., 0])\n",
    "plt.contour(y_train[n_sample//10, ..., 0], colors='r')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_train_aug[n_sample, ..., 0])\n",
    "plt.contour(y_train_aug[n_sample, ..., 0], colors='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def get_unet(input_img, n_filters = 8, dropout = 0.2, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    # Contracting Path\n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "\n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "\n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "\n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "\n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    smooth=1\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    from keras.losses import binary_crossentropy\n",
    "    return 0.5*keras.losses.binary_crossentropy(y_true,y_pred)+0.5*dice_loss(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input((im_height, im_width, 1), name='img')\n",
    "model = get_unet(input_img, n_filters=16, dropout=0.05, batchnorm=True)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=custom_loss, metrics=['accuracy', dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    #EarlyStopping(patience=10, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint('model_vessel-detection.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(X_train_aug, y_train_aug, \n",
    "                    batch_size=32, epochs=100, callbacks=callbacks, validation_data=(X_valid_aug, y_valid_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_vessel-detection.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_valid, y_valid, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on val and test\n",
    "#preds_test = model.predict(X_test, verbose=1)\n",
    "#preds_valid = model.predict(X_valid, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "#preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "#preds_valid_t = (preds_valid > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ix = 90\n",
    "\n",
    "#fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "#ax[0].imshow(X_valid[ix, ..., 0], cmap='viridis')\n",
    "#ax[0].contour(y_valid[ix].squeeze(), colors='r', levels=[0.5], linewidths=5)\n",
    "#ax[0].set_title('True')\n",
    "#ax[0].grid(False)\n",
    "\n",
    "#ax[1].imshow(X_valid[ix, ..., 0], cmap='viridis')\n",
    "#ax[1].contour(preds_valid[ix].squeeze(), colors='r', levels=[0.5], linewidths=5)\n",
    "#ax[1].set_title('Predicted')\n",
    "#ax[1].grid(False)\n",
    "\n",
    "#ax[2].imshow(y_valid[ix].squeeze(), vmin=0, vmax=2)\n",
    "#ax[2].contour(preds_valid[ix].squeeze(), colors='r', levels=[0.5], linewidths=5)\n",
    "#ax[2].set_title('Predicted segmentation')\n",
    "#ax[2].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_dcm = pydicom.read_file(img_names_test[0], force = True)\n",
    "img_dcm.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian\n",
    "img = img_dcm.pixel_array\n",
    "#img_min = np.min(img)\n",
    "#img_max = np.max(img)\n",
    "#img_norm = np.copy((img - img_min)/(img_max - img_min)*255).astype(np.uint8)\n",
    "\n",
    "#img_test = img_norm[8:-8,8:-8]\n",
    "img_test = img[8:-8,8:-8]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_test)\n",
    "for i in range (0, 8):\n",
    "    for j in range (0, 8):\n",
    "        plt.gca().add_patch(patches.Rectangle((64*i, 64*j), 64, 64, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "plt.axis('Off')\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(img_test)\n",
    "for i in range (0, 7):\n",
    "    for j in range (0, 7):\n",
    "        plt.gca().add_patch(patches.Rectangle((32+64*i, 32+64*j), 64, 64, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "plt.axis('Off')\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(img_test)\n",
    "for i in range (0, 5):\n",
    "    for j in range (0, 5):\n",
    "        plt.gca().add_patch(patches.Rectangle((16+96*i, 16+96*j), 96, 96, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_name = glob.glob(os.path.join(os.path.split(img_names_test[0])[0],\n",
    "                                  'MASSExport')+os.sep+'*'+ img_names_test[0].split(os.sep)[2][-10:-4]+'*.dcm')[0]\n",
    "con_test = path2array(con_name)[8:-8,8:-8]>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def detect4patches(img):\n",
    "    \n",
    "#    M_0 = np.zeros((512, 512), dtype = np.float)\n",
    "#    M_1 = np.zeros((512, 512), dtype = np.float)\n",
    "#    M_2 = np.zeros((512, 512), dtype = np.float)\n",
    "\n",
    "#    for i in range (0, 8):\n",
    "#        for j in range (0, 8):\n",
    "#            img_patch = np.zeros((1, 64, 64, 1))\n",
    "#            img_patch[0, ..., 0] = img[64*i:64*i+64, 64*j:64*j+64].copy()\n",
    "#            img_patch_pred = model.predict(img_patch, verbose=1)\n",
    "#            M_0[64*i:64*i+64, 64*j:64*j+64] = img_patch_pred[0, ..., 0]\n",
    "\n",
    "#    for i in range (0, 7):\n",
    "#        for j in range (0, 7):\n",
    "#            img_patch = np.zeros((1, 64, 64, 1))\n",
    "#            img_patch[0, ..., 0] = img[32+64*i:32+64*i+64, 32+64*j:32+64*j+64].copy()\n",
    "#            img_patch_pred = model.predict(img_patch, verbose=1)\n",
    "#            M_1[32+64*i:32+64*i+64, 32+64*j:32+64*j+64] = img_patch_pred[0, ..., 0]\n",
    "\n",
    "#    for i in range (0, 5):\n",
    "#        for j in range (0, 5):\n",
    "#            img_patch = np.zeros((1, 64, 64, 1))\n",
    "#            img_patch[0, ..., 0] = res = cv2.resize(img[16+96*i:16+96*i+96, 16+96*j:16+96*j+96].copy(),\n",
    "                                                    dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "#            mask_pred = model.predict(img_patch, verbose=1)\n",
    "#            img_patch_pred = cv2.resize(mask_pred[0, ..., 0],\n",
    "                                        dsize=(96, 96), interpolation=cv2.INTER_CUBIC)\n",
    "#            M_2[16+96*i:16+96*i+96, 16+96*j:16+96*j+96] = img_patch_pred\n",
    "            \n",
    "#    M_fuzed = (M_0+M_1+M_2)/3\n",
    "    \n",
    "#    return M_fuzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vessels_pred_simple = detect4patches(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_simple, colors = 'r')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vessels_pred_simple_th = vessels_pred_simple > 0.5\n",
    "#vessels_pred_simple_th_morph = morphology.remove_small_objects(vessels_pred_simple_th, 100, connectivity=3)\n",
    "\n",
    "#global_thresh = threshold_otsu(vessels_pred_simple)\n",
    "#vessels_pred_simple_otsu = (vessels_pred_simple>global_thresh)\n",
    "#vessels_pred_simple_otsu_morph = morphology.remove_small_objects(vessels_pred_simple_otsu, 100, connectivity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect4multipatches(img):\n",
    "    \n",
    "    img_min = np.min(img)\n",
    "    img_max = np.max(img)\n",
    "    \n",
    "    img_norm = ((img-img_min)/(img_max-img_min)*255).astype(np.uint8)\n",
    "    \n",
    "    steps = int((img.shape[0]-64)/4)+1\n",
    "    \n",
    "    M = np.empty((512, 512, steps*steps))\n",
    "    M[:] = np.NaN\n",
    "    img_patch = np.zeros((steps*steps, 64, 64, 1))\n",
    "    c = 0\n",
    "\n",
    "    for i in range (0, steps):\n",
    "        for j in range (0, steps):\n",
    "            \n",
    "            img_norm_crop = img_norm[4*i:4*i+64, 4*j:4*j+64]\n",
    "            img_patch[c, ..., 0] = img_norm_crop.copy()\n",
    "            c += 1\n",
    "            \n",
    "    img_patch_pred = model.predict(img_patch, verbose=1)\n",
    "    \n",
    "    c = 0\n",
    "    for i in range (0, steps):\n",
    "        for j in range (0, steps):\n",
    "            M[4*i:4*i+64, 4*j:4*j+64, c] = img_patch_pred[c, ..., 0]\n",
    "            c += 1\n",
    "            \n",
    "    M_concord = np.nanmean(M, axis = 2)\n",
    "    \n",
    "    del M\n",
    "            \n",
    "    return M_concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vessels_pred_multi, M = detect4multipatches(img_test)\n",
    "vessels_pred_multi = detect4multipatches(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(img_test)\n",
    "plt.contour(vessels_pred_multi, colors = 'r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20, 20))\n",
    "\n",
    "#plt.subplot(1, 4, 1)\n",
    "#plt.imshow(img_test)\n",
    "#for i in range (0, 1):\n",
    "#    for j in range (0, 1):\n",
    "#        plt.gca().add_patch(patches.Rectangle((64*i, 64*j), 64, 64, linewidth=1, edgecolor='r', facecolor='none'))\n",
    "\n",
    "#plt.subplot(1, 4, 2)\n",
    "#plt.imshow(M[..., 5000], vmin = 0, vmax = 1)\n",
    "\n",
    "#plt.subplot(1, 4, 3)\n",
    "#plt.imshow(M[..., 7500], vmin = 0, vmax = 1)\n",
    "\n",
    "#plt.subplot(1, 4, 4)\n",
    "#plt.imshow(vessels_pred_multi)\n",
    "        \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels_pred_multi_th = vessels_pred_multi > 0.7\n",
    "vessels_pred_multi_th_morph = morphology.remove_small_objects(vessels_pred_multi_th, 100, connectivity=3)\n",
    "\n",
    "global_thresh = threshold_otsu(vessels_pred_multi)\n",
    "vessels_pred_multi_otsu = (vessels_pred_multi>global_thresh)\n",
    "vessels_pred_multi_otsu_morph = morphology.remove_small_objects(vessels_pred_multi_otsu, 100, connectivity=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20, 10))\n",
    "\n",
    "#plt.subplot(2, 4, 1)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_simple_th, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 2)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_simple_th_morph, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 3)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_simple_otsu, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 4)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_simple_otsu_morph, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 5)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_multi_th, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 6)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_multi_th_morph, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 7)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_multi_otsu, colors = 'r')\n",
    "\n",
    "#plt.subplot(2, 4, 8)\n",
    "#plt.imshow(img_test)\n",
    "#plt.contour(vessels_pred_multi_otsu_morph, colors = 'r')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.imshow(img_test)\n",
    "plt.contour(con_test, colors = 'r')\n",
    "plt.contour(vessels_pred_multi_th_morph, colors = 'y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice(mask_1, mask_2):\n",
    "    intersection = np.sum(mask_1*mask_2)\n",
    "    sum_area = np.sum(mask_1) + np.sum(mask_2)\n",
    "    return 2*intersection/sum_area\n",
    "\n",
    "def calculate_overlap(mask_1, mask_2):\n",
    "    intersection = np.sum(mask_1*mask_2)\n",
    "    min_area = min(np.sum(mask_1), np.sum(mask_2))\n",
    "    return intersection/min_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_overlap = 0\n",
    "component_label = 0\n",
    "component_dice = 0\n",
    "\n",
    "vessels_pred_multi_th_morph_labeled = label(vessels_pred_multi_th_morph)\n",
    "\n",
    "for l in range(1, np.max(vessels_pred_multi_th_morph_labeled)+1):\n",
    "    l_mask = vessels_pred_multi_th_morph_labeled==l\n",
    "    d = calculate_dice(l_mask, con_test)\n",
    "    o = calculate_overlap(l_mask, con_test)\n",
    "    if o>max_overlap:\n",
    "        max_overlap = o\n",
    "        component_label = l\n",
    "        component_dice = d\n",
    "        \n",
    "if max_overlap > 0.5:\n",
    "    \n",
    "    vessel_mask = vessels_pred_multi_th_morph_labeled==component_label\n",
    "    \n",
    "    if np.sum(con_test)>0:\n",
    "        detected = 1\n",
    "        \n",
    "    print (max_overlap, component_dice, detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
